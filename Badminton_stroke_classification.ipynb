{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9380e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython import display\n",
    "from urllib import request\n",
    "from tensorflow_docs.vis import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2a0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import keras\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4045ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea20729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "    \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "    \"\"\"\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a58bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 5):\n",
    "    \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "    \"\"\"\n",
    "    # Read each video frame by frame\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    result.append(format_frames(frame, output_size))\n",
    "\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            frame = format_frames(frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            result.append(np.zeros_like(result[0]))\n",
    "    src.release()\n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5a3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/Users/nitu/Downloads/archive (1)/badminton_strokes_dataset/train/backhand_drive/bh_drive0.avi\"\n",
    "\n",
    "sample_video = frames_from_video_file(video_path, n_frames = 30)\n",
    "sample_video.shape\n",
    "\n",
    "def to_gif(images):\n",
    "    converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "    imageio.mimsave('./animation.gif', converted_images)\n",
    "    return embed.embed_file('./animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed5491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "    def __init__(self, path, n_frames, training = False):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "          Args:\n",
    "            path: Video file paths.\n",
    "            n_frames: Number of frames. \n",
    "            training: Boolean to determine if training dataset is being created.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "        self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "    def get_files_and_class_names(self):\n",
    "        video_paths = list(self.path.glob('*/*.avi'))\n",
    "        classes = [p.parent.name for p in video_paths] \n",
    "        return video_paths, classes\n",
    "\n",
    "    def __call__(self):\n",
    "        video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "        pairs = list(zip(video_paths, classes))\n",
    "\n",
    "        if self.training:\n",
    "            random.shuffle(pairs)\n",
    "\n",
    "        for path, name in pairs:\n",
    "            video_frames = frames_from_video_file(path, self.n_frames) \n",
    "            label = self.class_ids_for_name[name] # Encode labels\n",
    "            yield video_frames[5:15], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe67b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfa1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/Users/nitu/Downloads/archive (1)/badminton_strokes_dataset/train')\n",
    "class_names = sorted(set(p.name for p in path.iterdir() if p.is_dir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc7384e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = Path('/Users/nitu/Downloads/archive (1)/badminton_strokes_dataset/validation')\n",
    "\n",
    "path2 = Path('/Users/nitu/Downloads/archive (1)/badminton_strokes_dataset/test')\n",
    "\n",
    "subset_paths = {'train' : path,\n",
    "                'val' : path1,\n",
    "                'test' : path2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea7d9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backhand_drive',\n",
       " 'backhand_net_shot',\n",
       " 'forehand_clear',\n",
       " 'forehand_drive',\n",
       " 'forehand_lift',\n",
       " 'forehand_net_shot']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84ce923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids_for_name = dict((name, idx) for idx, name in enumerate(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c0bcb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backhand_drive': 0,\n",
       " 'backhand_net_shot': 1,\n",
       " 'forehand_clear': 2,\n",
       " 'forehand_drive': 3,\n",
       " 'forehand_lift': 4,\n",
       " 'forehand_net_shot': 5}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ids_for_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ee54fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': PosixPath('/Users/nitu/Downloads/archive (1)/badminton_strokes_dataset/train'),\n",
       " 'val': PosixPath('/Users/nitu/Downloads/archive (1)/badminton_strokes_dataset/validation'),\n",
       " 'test': PosixPath('/Users/nitu/Downloads/archive (1)/badminton_strokes_dataset/test')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f95748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10, 224, 224, 3)\n",
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "fg = FrameGenerator(subset_paths['train'], 20, training=True)\n",
    "\n",
    "frames, label = next(fg())\n",
    "\n",
    "print(f\"Shape: {frames.shape}\")\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffa638da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10, 224, 224, 3)\n",
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "frames, label = next(fg())\n",
    "\n",
    "print(f\"Shape: {frames.shape}\")\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "803015b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_frames = 20\n",
    "\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n",
    "                                          output_signature = output_signature)\n",
    "\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n",
    "                                         output_signature = output_signature)\n",
    "\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], num_frames),\n",
    "                                         output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dc503e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ds = train_ds.concatenate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e610528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "merged_ds = merged_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c6ae3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ds = merged_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1e6c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(batch_size)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6932f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb0c15f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:32:22.515003: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 41 of 1000\n",
      "2023-10-11 13:32:32.508181: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 85 of 1000\n",
      "2023-10-11 13:32:42.756878: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 131 of 1000\n",
      "2023-10-11 13:32:52.761434: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 173 of 1000\n",
      "2023-10-11 13:33:02.658387: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 215 of 1000\n",
      "2023-10-11 13:33:12.638258: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 261 of 1000\n",
      "2023-10-11 13:33:22.676950: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 304 of 1000\n",
      "2023-10-11 13:33:32.601385: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 350 of 1000\n",
      "2023-10-11 13:33:42.657291: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 394 of 1000\n",
      "2023-10-11 13:33:52.491707: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 435 of 1000\n",
      "2023-10-11 13:34:02.602473: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 477 of 1000\n",
      "2023-10-11 13:34:12.692541: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 520 of 1000\n",
      "2023-10-11 13:34:22.574942: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 566 of 1000\n",
      "2023-10-11 13:34:32.586778: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 610 of 1000\n",
      "2023-10-11 13:34:42.479533: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 655 of 1000\n",
      "2023-10-11 13:34:48.458230: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set of frames: (8, 10, 224, 224, 3)\n",
      "Shape of training labels: (8,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:34:58.512866: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 49 of 1000\n",
      "2023-10-11 13:35:08.557635: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 95 of 1000\n",
      "2023-10-11 13:35:18.654749: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 145 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set of frames: (8, 10, 224, 224, 3)\n",
      "Shape of validation labels: (8,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:35:22.599705: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "train_frames, train_labels = next(iter(merged_ds))\n",
    "print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "print(f'Shape of training labels: {train_labels.shape}')\n",
    "\n",
    "val_frames, val_labels = next(iter(val_ds))\n",
    "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
    "print(f'Shape of validation labels: {val_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b867d33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 4 0 4 1 4 4 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([4 1 3 4 4 3 4 4], shape=(8,), dtype=int16)\n",
      "tf.Tensor([2 2 4 2 3 1 3 1], shape=(8,), dtype=int16)\n",
      "tf.Tensor([3 2 1 1 1 4 0 4], shape=(8,), dtype=int16)\n",
      "tf.Tensor([5 3 1 4 5 0 2 1], shape=(8,), dtype=int16)\n",
      "tf.Tensor([5 2 5 1 1 4 3 1], shape=(8,), dtype=int16)\n",
      "tf.Tensor([5 1 4 5 4 3 1 5], shape=(8,), dtype=int16)\n",
      "tf.Tensor([1 4 4 5 2 1 3 4], shape=(8,), dtype=int16)\n",
      "tf.Tensor([1 4 4 3 3 2 2 5], shape=(8,), dtype=int16)\n",
      "tf.Tensor([1 3 1 1 0 5 3 2], shape=(8,), dtype=int16)\n"
     ]
    }
   ],
   "source": [
    "for frames, labels in merged_ds.take(10):\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bfbcfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8, 10, 224, 224, 3)\n",
      "Label: (8,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape: {frames.shape}\")\n",
    "print(f\"Label: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b46ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'a0'\n",
    "resolution = 224\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "backbone = movinet.Movinet(model_id=model_id)\n",
    "backbone.trainable = False\n",
    "\n",
    "# Set num_classes=600 to load the pre-trained weights from the original model\n",
    "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
    "model.build([None, None, None, None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87836889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x3fb711940>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained weights\n",
    "#!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
    "#!tar -xvf movinet_a0_base.tar.gz\n",
    "\n",
    "checkpoint_dir = f'movinet_{model_id}_base'\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "status = checkpoint.restore(checkpoint_path)\n",
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45413290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
    "    \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
    "    model = movinet_model.MovinetClassifier(backbone=backbone,num_classes=num_classes)\n",
    "    model.build([batch_size, num_frames, resolution, resolution, 3])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d916bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_classifier(batch_size, num_frames, resolution, backbone, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "037d0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 0.001)\n",
    "\n",
    "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "538ef7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:35:32.691696: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4061e2a20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-11 13:35:32.691716: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-10-11 13:35:32.723065: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 12s 12s/step - loss: 1.8119 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:35:38.824805: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 605s 7s/step - loss: 1.6578 - accuracy: 0.2988 - val_loss: 1.5085 - val_accuracy: 0.3875\n",
      "Epoch 2/4\n",
      "85/85 [==============================] - 598s 7s/step - loss: 1.3415 - accuracy: 0.4630 - val_loss: 1.3594 - val_accuracy: 0.4500\n",
      "Epoch 3/4\n",
      "85/85 [==============================] - 597s 7s/step - loss: 1.1178 - accuracy: 0.5680 - val_loss: 1.2340 - val_accuracy: 0.5500\n",
      "Epoch 4/4\n",
      "85/85 [==============================] - 596s 7s/step - loss: 0.9738 - accuracy: 0.6317 - val_loss: 1.2066 - val_accuracy: 0.5688\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(merged_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_freq=1,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bba1891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "85/85 [==============================] - 597s 7s/step - loss: 0.9075 - accuracy: 0.6494 - val_loss: 1.2616 - val_accuracy: 0.5063\n",
      "Epoch 2/2\n",
      "85/85 [==============================] - 600s 7s/step - loss: 0.8327 - accuracy: 0.6953 - val_loss: 1.3573 - val_accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "results2 = model.fit(merged_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs= 2,\n",
    "                    validation_freq=1,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c2143fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_predicted_labels(dataset):\n",
    "    \"\"\"\n",
    "    Create a list of actual ground truth values and the predictions from the model.\n",
    "\n",
    "    Args:\n",
    "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
    "\n",
    "    Return:\n",
    "      Ground truth and predicted values for a particular dataset.\n",
    "    \"\"\"\n",
    "    actual = [labels for _, labels in dataset.unbatch()]\n",
    "    predicted = model.predict(dataset)\n",
    "\n",
    "    actual = tf.stack(actual, axis=0)\n",
    "    predicted = tf.concat(predicted, axis=0)\n",
    "    predicted = tf.argmax(predicted, axis=1)\n",
    "\n",
    "    return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30244ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
    "    cm = tf.math.confusion_matrix(actual, predicted)\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "    sns.set(rc={'figure.figsize':(12, 12)})\n",
    "    sns.set(font_scale=1.4)\n",
    "    ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
    "    ax.set_xlabel('Predicted Action')\n",
    "    ax.set_ylabel('Actual Action')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b38a5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = FrameGenerator(subset_paths['train'], num_frames, training = True)\n",
    "label_names = list(fg.class_ids_for_name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = get_actual_predicted_labels(merged_ds)\n",
    "\n",
    "plot_confusion_matrix(actual, predicted, label_names, 'full_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
